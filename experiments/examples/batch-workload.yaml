extends: "../base/vllm-defaults.yaml"

name: "batch-workload-vllm"
description: "Batch workload with batch_size=8 on vLLM â€” measures throughput under batched dispatch"

model:
  name: "Qwen/Qwen3-0.6B"

infrastructure:
  gpu_type: "RTX 4000 Ada"
  gpu_count: 1

workload:
  type: "batch"
  batch_size: 8
  requests:
    source: "../../prompts/baseline.jsonl"
    count: 32
  parameters:
    max_tokens: 128
    temperature: 0.7
